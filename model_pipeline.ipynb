{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e91e8ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pokji/anaconda3/envs/cv_pytorch_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import timm           # Huggingface Pretrained Models\n",
    "import imageio        # Saving GIFs\n",
    "import time \n",
    "import os\n",
    "import wandb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08f09d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b33cf2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = timm.create_model('timm/vit_small_patch16_dinov3.lvd1689m', pretrained=True, features_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa99f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 384, 16, 16]),\n",
       " torch.Size([1, 384, 16, 16]),\n",
       " torch.Size([1, 384, 16, 16]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input = torch.randn(1, 3, 256, 256)\n",
    "encoder_output = encoder(example_input)\n",
    "feat_layer1, feat_layer2, feat_layer3 = encoder_output\n",
    "feat_layer1.shape, feat_layer2.shape, feat_layer3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adcc669",
   "metadata": {},
   "source": [
    "## Encoder and Decoder Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31d9090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example UNet decoder\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, encoder_dim, out_channels=1):\n",
    "        super().__init__()\n",
    "\n",
    "        def up_block(in_channels, out_channels):\n",
    "            return nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True), # Out Dims: [B, out_channels, H*2, W*2]\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), # Out Dims: [B, out_channels, H*2, W*2]\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        \n",
    "        self.up1 = up_block(encoder_dim, 128)               # out dims: [B, 128, 32, 32]\n",
    "        self.up2 = up_block(128, 64)                        # out dims: [B, 64, 64, 64]\n",
    "        self.up3 = up_block(64, 32)                         # out dims: [B, 32, 128, 128]\n",
    "        self.up4 = up_block(32, 16)                         # out dims: [B, 16, 256, 256]\n",
    "\n",
    "        # final head: predict 2 channels -> (delta_raw, mask_logit)\n",
    "        self.final_conv1 = nn.Conv2d(16, 16, 3, padding=1) # out dims: [B, 16, 256, 256]\n",
    "        self.final_relu = nn.ReLU(inplace=True)\n",
    "        # output 2 channels (delta, mask)\n",
    "        self.final_conv2 = nn.Conv2d(16, 2, 1)               # out dims: [B, 2, 256, 256]\n",
    "\n",
    "        # zero-init final bias so initial delta/mask ~ 0\n",
    "        nn.init.zeros_(self.final_conv2.bias)\n",
    "\n",
    "        # learnable global scale for the delta (starts small)\n",
    "        self.delta_scale = nn.Parameter(torch.tensor(4.0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up1(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.up3(x)\n",
    "        x = self.up4(x)\n",
    "        x = self.final_conv1(x)\n",
    "        x = self.final_relu(x)\n",
    "        x = self.final_conv2(x)   # shape: [B, 2, H, W]\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViT_UNet_NextFrame(nn.Module):\n",
    "    def __init__(self, vit_model='vit_small_patch16_224', in_channels=3, out_channels=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = timm.create_model(vit_model, pretrained=True, features_only=True)\n",
    "        encoder_channels_dim = self.encoder.feature_info[-1]['num_chs'] # get last feature map channels\n",
    "        self.decoder = ConvDecoder(encoder_channels_dim, out_channels)\n",
    "        \n",
    "        \n",
    "        # Freeze whole encoder\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Find and unfreeze PatchEmbed robustly\n",
    "        for name, module in self.encoder.named_modules():\n",
    "            if module.__class__.__name__ == \"PatchEmbed\" or \"patch_embed\" in name:\n",
    "                for p in module.parameters():\n",
    "                    p.requires_grad = True\n",
    "                print(\"Unfroze patch embed:\", name)\n",
    "                break\n",
    "\n",
    "        # Unfreeze last block + norm\n",
    "        for name, p in self.encoder.named_parameters():\n",
    "            if name.startswith(\"model.blocks.11.\"):  # last block\n",
    "                p.requires_grad = True\n",
    "                print(\"Unfroze last block:\", name)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        feats = self.encoder(x)[-1]   # last feature map\n",
    "        out = self.decoder(feats)     # [B,2,H,W]\n",
    "        delta_raw = out[:, :1, ...]   # raw delta logits\n",
    "        mask_logit = out[:, 1:2, ...] # mask logits\n",
    "\n",
    "        # constrained delta + mask\n",
    "        delta = torch.tanh(delta_raw) * self.decoder.delta_scale\n",
    "        mask_prob = torch.sigmoid(mask_logit)   # in [0,1]\n",
    "\n",
    "        return delta, mask_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc40c4e3",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e03521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowDataset(Dataset):\n",
    "    def __init__(self, seq_dirs, input_len=3, pred_len=1, transform=None):\n",
    "        self.seq_dirs = seq_dirs\n",
    "        self.input_len = input_len\n",
    "        self.pred_len = pred_len\n",
    "        self.window_size = input_len + pred_len\n",
    "        self.transform = transform\n",
    "\n",
    "        # collect frame paths and index map\n",
    "        self.seq_frame_paths = []\n",
    "        for d in self.seq_dirs:\n",
    "            files = sorted([f for f in os.listdir(d) if f.endswith('.png')])\n",
    "            self.seq_frame_paths.append([os.path.join(d, f) for f in files])\n",
    "\n",
    "        self.index_map = []\n",
    "        for seq_index, paths in enumerate(self.seq_frame_paths):\n",
    "            L = len(paths)\n",
    "            max_start = L - self.window_size + 1\n",
    "            if max_start > 0:\n",
    "                for start in range(max_start):\n",
    "                    self.index_map.append((seq_index, start))\n",
    "\n",
    "    def __len__(self):\n",
    "        length = len(self.index_map)\n",
    "        return length\n",
    "\n",
    "    def _load_frame(self, path):\n",
    "        frame = read_image(path, mode=ImageReadMode.GRAY)\n",
    "        frame = frame.float() / 255.0  # Normalize to [0,1]\n",
    "        if self.transform is not None:\n",
    "            frame = self.transform(frame)\n",
    "        return frame\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq_idx, start_idx = self.index_map[idx]\n",
    "        paths = self.seq_frame_paths[seq_idx]\n",
    "\n",
    "        frames = [self._load_frame(paths[i]) for i in range(start_idx, start_idx + self.window_size)]\n",
    "        input_frame = torch.cat(frames[:self.input_len], dim=0)  # (C=input_len,H,W)\n",
    "        last_frame = frames[self.input_len - 1]                # (1,H,W)\n",
    "        target_frame = frames[self.input_len]                  # (1,H,W)\n",
    "\n",
    "        return input_frame, last_frame, target_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ecbb6f",
   "metadata": {},
   "source": [
    "## Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2741469",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, criterion, device,\n",
    "                 scheduler=None, warmup_steps=500, min_lr_fraction=0.0,\n",
    "                 total_steps=None, clip_grad=None, save_path=None,\n",
    "                 scheduler_step_per_batch=False,\n",
    "                 early_stopping_patience=None, early_stopping_min_delta=0.0,\n",
    "                 early_stopping_restore_best=True, wandb_run=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.min_lr_fraction = min_lr_fraction\n",
    "        self.total_steps = total_steps\n",
    "        self.clip_grad = clip_grad\n",
    "        self.save_path = save_path\n",
    "        self.scheduler_step_per_batch = scheduler_step_per_batch\n",
    "        self.wandb = wandb_run\n",
    "\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.early_stopping_min_delta = early_stopping_min_delta\n",
    "        self.early_stopping_restore_best = early_stopping_restore_best\n",
    "\n",
    "        self.global_step = 0\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "    def _save_checkpoint(self, tag=\"last\", is_best=False):\n",
    "        if not self.save_path:\n",
    "            return\n",
    "        state = {\n",
    "            \"model_state\": self.model.state_dict(),\n",
    "            \"optim_state\": self.optimizer.state_dict(),\n",
    "            \"scheduler_state\": self.scheduler.state_dict() if self.scheduler is not None else None,\n",
    "            \"global_step\": self.global_step,\n",
    "        }\n",
    "        torch.save(state, f\"{self.save_path}_{tag}.pt\")\n",
    "        if is_best:\n",
    "            torch.save(state, f\"{self.save_path}_best.pt\")\n",
    "\n",
    "        # after saving file to disk (e.g. in _save_checkpoint)\n",
    "        if self.wandb and os.path.exists(f\"{self.save_path}_{tag}.pt\"):\n",
    "            art = wandb.Artifact(f\"model-{run.name}\", type=\"model\")\n",
    "            art.add_file(f\"{self.save_path}_{tag}.pt\")\n",
    "            self.wandb.log_artifact(art)\n",
    "\n",
    "\n",
    "    def load(self, path, map_location=None):\n",
    "        ckpt = torch.load(path, map_location=map_location)\n",
    "        self.model.load_state_dict(ckpt[\"model_state\"])\n",
    "        self.optimizer.load_state_dict(ckpt[\"optim_state\"])\n",
    "        if self.scheduler is not None and ckpt.get(\"scheduler_state\") is not None:\n",
    "            try:\n",
    "                self.scheduler.load_state_dict(ckpt[\"scheduler_state\"])\n",
    "            except Exception:\n",
    "                pass\n",
    "        self.global_step = ckpt.get(\"global_step\", 0)\n",
    "\n",
    "    \n",
    "    def _current_lrs(self):\n",
    "        return [group['lr'] for group in self.optimizer.param_groups]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gradient_l1(pred, target):\n",
    "        dx_p = pred[:, :, :, 1:] - pred[:, :, :, :-1]\n",
    "        dy_p = pred[:, :, 1:, :] - pred[:, :, :-1, :]\n",
    "        dx_t = target[:, :, :, 1:] - target[:, :, :, :-1]\n",
    "        dy_t = target[:, :, 1:, :] - target[:, :, :-1, :]\n",
    "        return (dx_p - dx_t).abs().mean() + (dy_p - dy_t).abs().mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def _tv(x):\n",
    "        return (x[:, :, :, 1:] - x[:, :, :, :-1]).abs().mean() + (x[:, :, 1:, :] - x[:, :, :-1, :]).abs().mean()\n",
    "    \n",
    "    def _train_one_epoch(self, train_loader, epoch, log_every=50):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        start = time.time()\n",
    "\n",
    "        loader = tqdm(train_loader, desc=f\"Train E{epoch}\", leave=False)\n",
    "        \n",
    "        for i, (inputs, lasts, targets) in enumerate(loader):\n",
    "            inputs = inputs.to(self.device)\n",
    "            lasts = lasts.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            delta, mask = self.model(inputs)\n",
    "            \n",
    "            pred = lasts * (1.0 - mask) + (lasts + delta) * mask\n",
    "\n",
    "            target_delta = targets - lasts\n",
    "            pred_delta = pred - lasts\n",
    "            delta_loss = F.l1_loss(pred_delta, target_delta)\n",
    "\n",
    "            motion = target_delta.abs()\n",
    "            mask_target = (motion > 0.01).float()  # threshold tune\n",
    "            mask_bce = F.binary_cross_entropy(mask, mask_target)\n",
    "            \n",
    "            loss = 2 * delta_loss + 0.2 * mask_bce\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            total_norm = 0.0\n",
    "            for p in self.model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** 0.5\n",
    "\n",
    "            if self.clip_grad is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip_grad)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.scheduler is not None and self.scheduler_step_per_batch:\n",
    "                try:\n",
    "                    self.scheduler.step()\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            self.global_step += 1\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "            # Log metrics to wandb\n",
    "            if self.wandb:\n",
    "                self.wandb.log({\n",
    "                    \"train/loss\": loss.item(),\n",
    "                    \"train/mask_mean\": mask.mean().item(),\n",
    "                    \"train/delta_mean\": delta.abs().mean().item(),\n",
    "                    \"train/delta_std\": delta.std().item(),\n",
    "                    \"train/lr\": self._current_lrs()[0],\n",
    "                    \"train/grad_norm\": total_norm,\n",
    "                }, step=self.global_step)\n",
    "\n",
    "            # log example images occasionally (e.g. every 200 steps)\n",
    "            if self.wandb and (self.global_step % 200 == 0):\n",
    "                # select first example in batch\n",
    "                def to_img(t):\n",
    "                    return (t.squeeze(0).detach().cpu().numpy() * 255).astype(\"uint8\")\n",
    "                img_last = to_img(lasts[0])\n",
    "                img_pred = to_img(pred[0])\n",
    "                img_tgt  = to_img(targets[0])\n",
    "                self.wandb.log({\n",
    "                    \"train/visuals\": [\n",
    "                        wandb.Image(img_last, caption=\"last\"),\n",
    "                        wandb.Image(img_pred,  caption=\"pred\"),\n",
    "                        wandb.Image(img_tgt,   caption=\"target\"),\n",
    "                    ]\n",
    "                }, step=self.global_step)\n",
    "\n",
    "\n",
    "            if (i + 1) % log_every == 0:\n",
    "                avg = running_loss / float(i + 1)\n",
    "                lrs = self._current_lrs()\n",
    "                if len(lrs) == 1:\n",
    "                    lr_info = f\"lr={lrs[0]:.3e}\"\n",
    "                else:\n",
    "                    lr_info = \"lrs=\" + \",\".join(f\"{x:.3e}\" for x in lrs)\n",
    "                loader.set_postfix(loss=f\"{avg:.6f}\", lr=lr_info)\n",
    "                print(f\"Epoch {epoch} Step {i+1}/{len(train_loader)}  loss={avg:.6f}  {lr_info}\")\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        epoch_loss = running_loss / max(1, len(train_loader))\n",
    "        lrs = self._current_lrs()\n",
    "        if len(lrs) == 1:\n",
    "            lr_info = f\"lr={lrs[0]:.3e}\"\n",
    "        else:\n",
    "            lr_info = \"lrs=\" + \",\".join(f\"{x:.3e}\" for x in lrs)\n",
    "        print(f\"Epoch {epoch} completed in {elapsed:.1f}s  avg_loss={epoch_loss:.6f}  {lr_info}\")\n",
    "        return epoch_loss\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        total = 0.0\n",
    "        count = 0\n",
    "        for inputs, lasts, targets in tqdm(val_loader, desc=\"Val\", leave=False):\n",
    "            inputs = inputs.to(self.device)\n",
    "            lasts = lasts.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "            delta, mask = self.model(inputs)\n",
    "            pred = lasts * (1.0 - mask) + (lasts + delta) * mask\n",
    "            loss = self.criterion(pred, targets)\n",
    "            total += loss.item()\n",
    "            count += 1\n",
    "        return total / max(1, count)\n",
    "\n",
    "    def fit(self, train_loader, epochs=10, val_loader=None, log_every=50, save_every=1):\n",
    "        history = {\"train_loss\": [], \"val_loss\": []}\n",
    "        best_val = float('inf')\n",
    "        best_ckpt = None\n",
    "        no_improve = 0\n",
    "        best_epoch = None\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train_loss = self._train_one_epoch(train_loader, epoch, log_every=log_every)\n",
    "            history[\"train_loss\"].append(train_loss)\n",
    "\n",
    "            if self.scheduler is not None and not self.scheduler_step_per_batch:\n",
    "                try:\n",
    "                    self.scheduler.step()\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            if val_loader is not None:\n",
    "                val_loss = self.validate(val_loader)\n",
    "                history[\"val_loss\"].append(val_loss)\n",
    "                print(f\"Validation loss: {val_loss:.6f}\")\n",
    "\n",
    "\n",
    "                if self.wandb:\n",
    "                    self.wandb.log({\"val/loss\": val_loss}, step=self.global_step)\n",
    "\n",
    "                improved = (val_loss + self.early_stopping_min_delta) < best_val\n",
    "                if improved:\n",
    "                    best_val = val_loss\n",
    "                    no_improve = 0\n",
    "                    best_epoch = epoch\n",
    "                    # save best checkpoint\n",
    "                    if self.save_path:\n",
    "                        self._save_checkpoint(tag=f\"epoch{epoch}\", is_best=True)\n",
    "                    # keep best state in memory to optionally restore without file IO\n",
    "                    best_ckpt = {\n",
    "                        \"model_state\": self.model.state_dict(),\n",
    "                        \"optim_state\": self.optimizer.state_dict(),\n",
    "                        \"scheduler_state\": self.scheduler.state_dict() if self.scheduler is not None else None,\n",
    "                        \"global_step\": self.global_step,\n",
    "                    }\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "\n",
    "                if (self.early_stopping_patience is not None) and (no_improve >= self.early_stopping_patience):\n",
    "                    print(f\"Early stopping triggered (no improvement for {no_improve} epochs).\")\n",
    "                    break\n",
    "\n",
    "            if self.save_path and (epoch % save_every == 0 or epoch == epochs):\n",
    "                self._save_checkpoint(tag=f\"epoch{epoch}\")\n",
    "\n",
    "        if self.save_path:\n",
    "            self._save_checkpoint(tag=\"final\")\n",
    "\n",
    "        \n",
    "\n",
    "        if self.early_stopping_restore_best and best_ckpt is not None:\n",
    "            try:\n",
    "                self.model.load_state_dict(best_ckpt[\"model_state\"])\n",
    "                self.optimizer.load_state_dict(best_ckpt[\"optim_state\"])\n",
    "                if self.scheduler is not None and best_ckpt.get(\"scheduler_state\") is not None:\n",
    "                    try:\n",
    "                        self.scheduler.load_state_dict(best_ckpt[\"scheduler_state\"])\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                print(f\"Restored best model from epoch {best_epoch} with val_loss={best_val:.6f}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f5a3a8",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3360d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ec8958",
   "metadata": {},
   "source": [
    "### Initialize model, criterion and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf357f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfroze patch embed: model.patch_embed\n"
     ]
    }
   ],
   "source": [
    "# Model, loss, optimizer\n",
    "model = ViT_UNet_NextFrame(vit_model='timm/vit_small_patch16_dinov3.lvd1689m', in_channels=3, out_channels=1)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7c10a8",
   "metadata": {},
   "source": [
    "### Split Dataset, Initialize DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6296048b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "seq_dirs = sorted(glob.glob(\"bouncing_balls_dataset/seq*\"))\n",
    "len(seq_dirs[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51373b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "\n",
    "seq_dirs = sorted(glob.glob(\"bouncing_balls_dataset/seq*\"))\n",
    "\n",
    "\n",
    "# deterministic shuffle + split (80/10/10)\n",
    "random.Random(42).shuffle(seq_dirs)\n",
    "n = len(seq_dirs)\n",
    "train_end = int(0.8 * n)\n",
    "val_end = train_end + int(0.1 * n)\n",
    "\n",
    "# Split directories\n",
    "train_dirs = seq_dirs[:train_end]\n",
    "val_dirs   = seq_dirs[train_end:val_end]\n",
    "test_dirs  = seq_dirs[val_end:]\n",
    "\n",
    "# Create datasets\n",
    "train_ds = SlidingWindowDataset(train_dirs, input_len=3, pred_len=1)\n",
    "val_ds   = SlidingWindowDataset(val_dirs, input_len=3, pred_len=1)\n",
    "test_ds  = SlidingWindowDataset(test_dirs, input_len=3, pred_len=1)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed577675",
   "metadata": {},
   "source": [
    "## Wandb Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5947fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">whole-field-16</strong> at: <a href='https://wandb.ai/bartosztarnachowicz1-poznan-university-of-technology/video-generation-project/runs/7d948edx' target=\"_blank\">https://wandb.ai/bartosztarnachowicz1-poznan-university-of-technology/video-generation-project/runs/7d948edx</a><br> View project at: <a href='https://wandb.ai/bartosztarnachowicz1-poznan-university-of-technology/video-generation-project' target=\"_blank\">https://wandb.ai/bartosztarnachowicz1-poznan-university-of-technology/video-generation-project</a><br>Synced 8 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260122_105702-7d948edx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/pokji/university/sem5/CV/video-generation-project/wandb/run-20260122_105845-jjxts1ua</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bartosztarnachowicz1-poznan-university-of-technology/video-generation-project/runs/jjxts1ua' target=\"_blank\">young-cherry-17</a></strong> to <a href='https://wandb.ai/bartosztarnachowicz1-poznan-university-of-technology/video-generation-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bartosztarnachowicz1-poznan-university-of-technology/video-generation-project' target=\"_blank\">https://wandb.ai/bartosztarnachowicz1-poznan-university-of-technology/video-generation-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bartosztarnachowicz1-poznan-university-of-technology/video-generation-project/runs/jjxts1ua' target=\"_blank\">https://wandb.ai/bartosztarnachowicz1-poznan-university-of-technology/video-generation-project/runs/jjxts1ua</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_config = dict(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=LR,\n",
    "    epochs=EPOCHS,\n",
    "    input_len=3,\n",
    ")\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"video-generation-project\",\n",
    "    job_type=\"train\",\n",
    "    config=wandb_config,\n",
    "    save_code=True\n",
    ")\n",
    "\n",
    "wandb.watch(model, log=\"all\", log_freq=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce031582",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb317039",
   "metadata": {},
   "outputs": [],
   "source": [
    "WARMUP_STEPS = len(train_loader) * 1  # 1 epoch warmup\n",
    "TOTAL_STEPS = EPOCHS * len(train_loader)\n",
    "MIN_LR = 0.0\n",
    "\n",
    "warmup_scheduler = LinearLR(optimizer, start_factor=1e-6, total_iters=WARMUP_STEPS)\n",
    "cosine_scheduler = CosineAnnealingLR(optimizer, T_max=max(1, TOTAL_STEPS - WARMUP_STEPS), eta_min=MIN_LR)\n",
    "scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[WARMUP_STEPS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ea75754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train E1:   0%|          | 0/940 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pixel_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, optimizer, criterion, device,\n\u001b[1;32m      2\u001b[0m                   scheduler\u001b[38;5;241m=\u001b[39mscheduler, scheduler_step_per_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      3\u001b[0m                   clip_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, wandb_run\u001b[38;5;241m=\u001b[39mrun)\n\u001b[0;32m----> 5\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 170\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, train_loader, epochs, val_loader, log_every, save_every)\u001b[0m\n\u001b[1;32m    167\u001b[0m best_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 170\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_every\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler_step_per_batch:\n",
      "Cell \u001b[0;32mIn[5], line 102\u001b[0m, in \u001b[0;36mTrainer._train_one_epoch\u001b[0;34m(self, train_loader, epoch, log_every)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Log metrics to wandb\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwandb:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwandb\u001b[38;5;241m.\u001b[39mlog({\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[0;32m--> 102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/pixel_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mpixel_loss\u001b[49m\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/mask_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m: mask\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/tv\u001b[39m\u001b[38;5;124m\"\u001b[39m: tv\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/lr\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_lrs()[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    106\u001b[0m     }, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_step)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# log example images occasionally (e.g. every 200 steps)\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwandb \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# select first example in batch\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pixel_loss' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, optimizer, criterion, device,\n",
    "                  scheduler=scheduler, scheduler_step_per_batch=True,\n",
    "                  clip_grad=1.0, wandb_run=run)\n",
    "\n",
    "history = trainer.fit(train_loader=train_loader, val_loader=val_loader,\n",
    "                      epochs=EPOCHS, log_every=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23ca642",
   "metadata": {},
   "source": [
    "## Testing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9535ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "def generate_recursive(model, init_inputs, steps, device, cv_enhance=False):\n",
    "    \"\"\"\n",
    "    model: your ViT_UNet_NextFrame (expects input shape (B,C,H,W))\n",
    "    init_inputs: tensor (B,C,H,W) with C = input_len (use unsqueeze(0) for single example)\n",
    "    steps: number of future frames to generate\n",
    "    returns: list of predicted frames (as tensors, each (H,W) in [0,1])\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    frames = []\n",
    "    cur = init_inputs.to(device).clone()\n",
    "\n",
    "    if cv_enhance:\n",
    "        kernel = np.ones((2, 2), np.uint8)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(steps):\n",
    "            delta, mask = model(cur)\n",
    "            last = cur[:, -1:, :, :]\n",
    "            pred = last * (1.0 - mask) + (last + delta) * mask\n",
    "\n",
    "            if cv_enhance:\n",
    "                # morph close on binarized prediction\n",
    "                pred_np = pred[0,0].detach().cpu().numpy()\n",
    "                pred_np = cv2.medianBlur(pred_np.astype(np.float32), 3)\n",
    "                pred_bin = (pred_np > 0.2).astype(np.uint8)\n",
    "                \n",
    "                # after pred_bin computed\n",
    "                num, labels, stats, _ = cv2.connectedComponentsWithStats(pred_bin, connectivity=4)\n",
    "                # stats: [label, x, y, w, h, area]\n",
    "                min_area = 200  # tune\n",
    "                mask = np.zeros_like(pred_bin)\n",
    "                for i in range(1, num):  # skip background\n",
    "                    if stats[i, cv2.CC_STAT_AREA] >= min_area:\n",
    "                        mask[labels == i] = 1\n",
    "                pred_clean = mask.astype(np.uint8)\n",
    "    \n",
    "                pred = torch.from_numpy(pred_clean).float().to(device).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            frames.append(pred[0].squeeze(0).cpu())\n",
    "            cur = torch.cat([cur[:, 1:, :, :], pred], dim=1)\n",
    "    return frames\n",
    "\n",
    "\n",
    "def save_frames_as_gif(frames, path, fps=10, loop=0):\n",
    "    \"\"\"\n",
    "    frames: list of (H,W) tensors in [0,1]\n",
    "    path: output filepath (e.g. 'pred.gif')\n",
    "    fps: frames per second\n",
    "    loop: number of loops (0 = infinite)\n",
    "    \"\"\"\n",
    "    \n",
    "    out = []\n",
    "    for frame in frames:\n",
    "        # Convert tensor to numpy array and scale to [0,255]\n",
    "        arr = (frame.squeeze().cpu().numpy() * 255).astype('uint8')\n",
    "        out.append(arr)\n",
    "    imageio.mimsave(path, out, format='GIF', fps=fps, loop=loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97127d0",
   "metadata": {},
   "source": [
    "### Generate video in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af054015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6818/3695032471.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"model.pt\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfroze patch embed: model.patch_embed\n",
      "Unfroze last block: model.blocks.11.gamma_1\n",
      "Unfroze last block: model.blocks.11.gamma_2\n",
      "Unfroze last block: model.blocks.11.norm1.weight\n",
      "Unfroze last block: model.blocks.11.norm1.bias\n",
      "Unfroze last block: model.blocks.11.attn.qkv.weight\n",
      "Unfroze last block: model.blocks.11.attn.proj.weight\n",
      "Unfroze last block: model.blocks.11.attn.proj.bias\n",
      "Unfroze last block: model.blocks.11.norm2.weight\n",
      "Unfroze last block: model.blocks.11.norm2.bias\n",
      "Unfroze last block: model.blocks.11.mlp.fc1.weight\n",
      "Unfroze last block: model.blocks.11.mlp.fc1.bias\n",
      "Unfroze last block: model.blocks.11.mlp.fc2.weight\n",
      "Unfroze last block: model.blocks.11.mlp.fc2.bias\n"
     ]
    }
   ],
   "source": [
    "ckpt = torch.load(\"model.pt\", map_location=device)\n",
    "model = ViT_UNet_NextFrame(vit_model='timm/vit_small_patch16_dinov3.lvd1689m', in_channels=3, out_channels=1).to(device)\n",
    "model.load_state_dict(ckpt)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "13736b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "steps = 25\n",
    "idx = 500\n",
    "init_input, _, _ = test_ds[idx]\n",
    "\n",
    "frames = generate_recursive(model, init_input.unsqueeze(0), steps, device, cv_enhance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "201424dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for frame in frames[:100]:\n",
    "    clear_output(wait=True)\n",
    "    plt.imshow(frame.squeeze(0), cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis('off')\n",
    "    display(plt.gcf())\n",
    "    plt.pause(0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e1d497",
   "metadata": {},
   "source": [
    "### Save to gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c55dd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_frames_as_gif(frames, \"predicted.gif\", fps=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
