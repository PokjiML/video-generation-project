{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91e8ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from IPython.display import display\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f09d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d9090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example UNet decoder\n",
    "class SimpleDecoder(nn.Module):\n",
    "    def __init__(self, encoder_dim, out_channels=1):\n",
    "        super().__init__()\n",
    "\n",
    "        def up_block(in_channels, out_channels):\n",
    "            return nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True), # Out Dims: [B, out_channels, H*2, W*2]\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), # Out Dims: [B, out_channels, H*2, W*2]\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        \n",
    "        self.up1 = up_block(encoder_dim, 128)               # out dims: [B, 128, 32, 32]\n",
    "        self.up2 = up_block(128, 64)                        # out dims: [B, 64, 64, 64]\n",
    "        self.up3 = up_block(64, 32)                         # out dims: [B, 32, 128, 128]\n",
    "        self.up4 = up_block(32, 16)                         # out dims: [B, 16, 256, 256]\n",
    "\n",
    "        # final head: predict 2 channels -> (delta_raw, mask_logit)\n",
    "        self.final_conv1 = nn.Conv2d(16, 16, 3, padding=1)\n",
    "        self.final_relu = nn.ReLU(inplace=True)\n",
    "        # output 2 channels (delta, mask)\n",
    "        self.final_conv2 = nn.Conv2d(16, 2, 1)\n",
    "\n",
    "        # zero-init final bias so initial delta/mask ~ 0\n",
    "        nn.init.zeros_(self.final_conv2.bias)\n",
    "\n",
    "        # learnable global scale for the delta (starts small)\n",
    "        self.delta_scale = nn.Parameter(torch.tensor(0.1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up1(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.up3(x)\n",
    "        x = self.up4(x)\n",
    "        x = self.final_conv1(x)\n",
    "        x = self.final_relu(x)\n",
    "        x = self.final_conv2(x)   # shape: [B, 2, H, W]\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViT_UNet_NextFrame(nn.Module):\n",
    "    def __init__(self, vit_model='vit_small_patch16_224', in_channels=3, out_channels=1):\n",
    "        super().__init__()\n",
    "        # ViT expects 3 channels, so we use a 1x1 conv to map stacked frames to 3 channels\n",
    "        self.encoder = timm.create_model(vit_model, pretrained=True, features_only=True)\n",
    "        encoder_dim = self.encoder.feature_info[-1]['num_chs']\n",
    "        self.decoder = SimpleDecoder(encoder_dim, out_channels)\n",
    "        \n",
    "        \n",
    "        # Freeze whole encoder\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Find and unfreeze PatchEmbed robustly\n",
    "        for name, module in self.encoder.named_modules():\n",
    "            if module.__class__.__name__ == \"PatchEmbed\" or \"patch_embed\" in name:\n",
    "                for p in module.parameters():\n",
    "                    p.requires_grad = True\n",
    "                print(\"Unfroze patch embed:\", name)\n",
    "                break\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        feats = self.encoder(x)[-1]   # last feature map\n",
    "        out = self.decoder(feats)     # [B,2,H,W]\n",
    "        delta_raw = out[:, :1, ...]   # raw delta logits\n",
    "        mask_logit = out[:, 1:2, ...] # mask logits\n",
    "\n",
    "        # constrained delta + mask\n",
    "        delta = torch.tanh(delta_raw) * self.decoder.delta_scale\n",
    "        mask = torch.sigmoid(mask_logit)   # in [0,1]\n",
    "\n",
    "        return delta, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f525c321",
   "metadata": {},
   "source": [
    "### Load the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1a79d902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def load_frames(frame_dir, transform=None):\n",
    "    if transform is None:\n",
    "        transform = transforms.ToTensor()  # converts PIL L -> tensor shape (1,H,W)\n",
    "    frame_files = sorted([f for f in os.listdir(frame_dir) if f.endswith('.png')])\n",
    "    frames = []\n",
    "    for f in frame_files:\n",
    "        img = Image.open(os.path.join(frame_dir, f)).convert('L')\n",
    "        frames.append(transform(img))\n",
    "    return frames\n",
    "\n",
    "# call:\n",
    "frames = load_frames(\"bouncing_ball_dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc40c4e3",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd9e24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FramesDataset(Dataset):\n",
    "    def __init__(self, frames, num_input_frames=3, transforms=None):\n",
    "        # frames: list of tensors each shape (1,H,W)\n",
    "        self.frames = frames\n",
    "        self.n = num_input_frames\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.samples = []\n",
    "        for i in range(len(frames) - (self.n)):\n",
    "            inp = torch.cat(frames[i:i+self.n], dim=0)   # (C=num_input_frames, H, W)\n",
    "            last = frames[i+self.n-1]                    # (1,H,W)\n",
    "            target = frames[i+self.n]                    # (1,H,W)\n",
    "            self.samples.append((inp, last, target))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ecbb6f",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2741469",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, criterion, device,\n",
    "                 scheduler=None, warmup_steps=500, min_lr_fraction=0.0,\n",
    "                 total_steps=None, clip_grad=None, save_path=None,\n",
    "                 scheduler_step_per_batch=False,\n",
    "                 early_stopping_patience=None, early_stopping_min_delta=0.0,\n",
    "                 early_stopping_restore_best=True):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.min_lr_fraction = min_lr_fraction\n",
    "        self.total_steps = total_steps\n",
    "        self.clip_grad = clip_grad\n",
    "        self.save_path = save_path\n",
    "        self.scheduler_step_per_batch = scheduler_step_per_batch\n",
    "\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.early_stopping_min_delta = early_stopping_min_delta\n",
    "        self.early_stopping_restore_best = early_stopping_restore_best\n",
    "\n",
    "        self.global_step = 0\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "    def _save_checkpoint(self, tag=\"last\", is_best=False):\n",
    "        if not self.save_path:\n",
    "            return\n",
    "        state = {\n",
    "            \"model_state\": self.model.state_dict(),\n",
    "            \"optim_state\": self.optimizer.state_dict(),\n",
    "            \"scheduler_state\": self.scheduler.state_dict() if self.scheduler is not None else None,\n",
    "            \"global_step\": self.global_step,\n",
    "        }\n",
    "        torch.save(state, f\"{self.save_path}_{tag}.pt\")\n",
    "        if is_best:\n",
    "            torch.save(state, f\"{self.save_path}_best.pt\")\n",
    "\n",
    "    def load(self, path, map_location=None):\n",
    "        ckpt = torch.load(path, map_location=map_location)\n",
    "        self.model.load_state_dict(ckpt[\"model_state\"])\n",
    "        self.optimizer.load_state_dict(ckpt[\"optim_state\"])\n",
    "        if self.scheduler is not None and ckpt.get(\"scheduler_state\") is not None:\n",
    "            try:\n",
    "                self.scheduler.load_state_dict(ckpt[\"scheduler_state\"])\n",
    "            except Exception:\n",
    "                pass\n",
    "        self.global_step = ckpt.get(\"global_step\", 0)\n",
    "\n",
    "    def _current_lrs(self):\n",
    "        return [group['lr'] for group in self.optimizer.param_groups]\n",
    "\n",
    "    def _train_one_epoch(self, train_loader, epoch, log_every=50):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        start = time.time()\n",
    "        for i, (inputs, lasts, targets) in enumerate(train_loader):\n",
    "            inputs = inputs.to(self.device)\n",
    "            lasts = lasts.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            delta, mask = self.model(inputs)\n",
    "            pred = lasts * (1.0 - mask) + (lasts + delta) * mask\n",
    "            loss = self.criterion(pred, targets)\n",
    "            loss = loss + 0.01 * mask.mean()\n",
    "\n",
    "            loss.backward()\n",
    "            if self.clip_grad is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip_grad)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.scheduler is not None and self.scheduler_step_per_batch:\n",
    "                try:\n",
    "                    self.scheduler.step()\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            self.global_step += 1\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (i + 1) % log_every == 0:\n",
    "                avg = running_loss / float(i + 1)\n",
    "                lrs = self._current_lrs()\n",
    "                if len(lrs) == 1:\n",
    "                    lr_info = f\"lr={lrs[0]:.3e}\"\n",
    "                else:\n",
    "                    lr_info = \"lrs=\" + \",\".join(f\"{x:.3e}\" for x in lrs)\n",
    "                print(f\"Epoch {epoch} Step {i+1}/{len(train_loader)}  loss={avg:.6f}  {lr_info}\")\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        epoch_loss = running_loss / max(1, len(train_loader))\n",
    "        lrs = self._current_lrs()\n",
    "        if len(lrs) == 1:\n",
    "            lr_info = f\"lr={lrs[0]:.3e}\"\n",
    "        else:\n",
    "            lr_info = \"lrs=\" + \",\".join(f\"{x:.3e}\" for x in lrs)\n",
    "        print(f\"Epoch {epoch} completed in {elapsed:.1f}s  avg_loss={epoch_loss:.6f}  {lr_info}\")\n",
    "        return epoch_loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        tot = 0.0\n",
    "        count = 0\n",
    "        for inputs, lasts, targets in val_loader:\n",
    "            inputs = inputs.to(self.device)\n",
    "            lasts = lasts.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "            delta, mask = self.model(inputs)\n",
    "            pred = lasts * (1.0 - mask) + (lasts + delta) * mask\n",
    "            loss = self.criterion(pred, targets)\n",
    "            tot += loss.item()\n",
    "            count += 1\n",
    "        return tot / max(1, count)\n",
    "\n",
    "    def fit(self, train_loader, epochs=10, val_loader=None, log_every=50, save_every=1):\n",
    "        history = {\"train_loss\": [], \"val_loss\": []}\n",
    "        best_val = float('inf')\n",
    "        best_ckpt = None\n",
    "        no_improve = 0\n",
    "        best_epoch = None\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train_loss = self._train_one_epoch(train_loader, epoch, log_every=log_every)\n",
    "            history[\"train_loss\"].append(train_loss)\n",
    "\n",
    "            if self.scheduler is not None and not self.scheduler_step_per_batch:\n",
    "                try:\n",
    "                    self.scheduler.step()\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            if val_loader is not None:\n",
    "                val_loss = self.validate(val_loader)\n",
    "                history[\"val_loss\"].append(val_loss)\n",
    "                print(f\"Validation loss: {val_loss:.6f}\")\n",
    "\n",
    "                improved = (val_loss + self.early_stopping_min_delta) < best_val\n",
    "                if improved:\n",
    "                    best_val = val_loss\n",
    "                    no_improve = 0\n",
    "                    best_epoch = epoch\n",
    "                    # save best checkpoint\n",
    "                    if self.save_path:\n",
    "                        self._save_checkpoint(tag=f\"epoch{epoch}\", is_best=True)\n",
    "                    # keep best state in memory to optionally restore without file IO\n",
    "                    best_ckpt = {\n",
    "                        \"model_state\": self.model.state_dict(),\n",
    "                        \"optim_state\": self.optimizer.state_dict(),\n",
    "                        \"scheduler_state\": self.scheduler.state_dict() if self.scheduler is not None else None,\n",
    "                        \"global_step\": self.global_step,\n",
    "                    }\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "\n",
    "                if self.early_stopping_patience is not None and no_improve >= self.early_stopping_patience:\n",
    "                    print(f\"Early stopping triggered (no improvement for {no_improve} epochs).\")\n",
    "                    break\n",
    "\n",
    "            if self.save_path and (epoch % save_every == 0 or epoch == epochs):\n",
    "                self._save_checkpoint(tag=f\"epoch{epoch}\")\n",
    "\n",
    "        if self.save_path:\n",
    "            self._save_checkpoint(tag=\"final\")\n",
    "\n",
    "        if self.early_stopping_restore_best and best_ckpt is not None:\n",
    "            try:\n",
    "                self.model.load_state_dict(best_ckpt[\"model_state\"])\n",
    "                self.optimizer.load_state_dict(best_ckpt[\"optim_state\"])\n",
    "                if self.scheduler is not None and best_ckpt.get(\"scheduler_state\") is not None:\n",
    "                    try:\n",
    "                        self.scheduler.load_state_dict(best_ckpt[\"scheduler_state\"])\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                print(f\"Restored best model from epoch {best_epoch} with val_loss={best_val:.6f}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf357f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, loss, optimizer\n",
    "model = ViT_UNet_NextFrame(vit_model='vit_small_patch16_224', in_channels=3, out_channels=1)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "\n",
    "# # Load dataset and dataloader\n",
    "# dataset = FramesDataset(frames, num_input_frames=3)\n",
    "# loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)  # adjust batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5b1f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sorted png paths (no image loaded yet)\n",
    "paths = list_frame_paths(\"bouncing_ball_dataset\")\n",
    "\n",
    "# create lazy dataset (FramesDataset uses Image.open in __getitem__)\n",
    "dataset = FramesDataset(paths, num_input_frames=3)\n",
    "\n",
    "# deterministic train/val split\n",
    "n = len(dataset)\n",
    "val_len = int(0.1 * n)\n",
    "train_len = n - val_len\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_ds, val_ds = torch.utils.data.random_split(dataset, [train_len, val_len], generator=generator)\n",
    "\n",
    "# DataLoader: start with small batch and increase until OOM risk\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True,\n",
    "                          num_workers=4, pin_memory=True,\n",
    "                          persistent_workers=True, prefetch_factor=2)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=8, shuffle=False,\n",
    "                          num_workers=2, pin_memory=True,\n",
    "                          persistent_workers=True, prefetch_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27494faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train/val\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "val_frac = 0.1\n",
    "n = len(dataset)\n",
    "val_len = int(n * val_frac)\n",
    "train_len = n - val_len\n",
    "train_ds, val_ds = random_split(dataset, [train_len, val_len])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "# instantiate trainer (you already have Trainer that accepts val_loader)\n",
    "trainer = Trainer(model, optimizer, criterion, device,\n",
    "                  scheduler=scheduler, scheduler_step_per_batch=True,\n",
    "                  clip_grad=1.0)\n",
    "\n",
    "history = trainer.fit(train_loader=train_loader, epochs=epochs, val_loader=val_loader, log_every=20)\n",
    "\n",
    "# inspect / plot validation loss\n",
    "print(history['val_loss'])\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history['train_loss'], label='train')\n",
    "plt.plot(history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b76a5a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 20/32  loss=0.018014  lr=4.000e-05\n",
      "Epoch 1 completed in 28.6s  avg_loss=0.017747  lr=6.400e-05\n",
      "Epoch 2 Step 20/32  loss=0.016875  lr=1.040e-04\n",
      "Epoch 2 completed in 28.8s  avg_loss=0.016727  lr=1.280e-04\n",
      "Epoch 3 Step 20/32  loss=0.016121  lr=1.680e-04\n",
      "Epoch 3 completed in 28.9s  avg_loss=0.015938  lr=1.920e-04\n",
      "Epoch 4 Step 20/32  loss=0.015016  lr=2.320e-04\n",
      "Epoch 4 completed in 30.7s  avg_loss=0.014704  lr=2.560e-04\n",
      "Epoch 5 Step 20/32  loss=0.013488  lr=2.960e-04\n",
      "Epoch 5 completed in 53.7s  avg_loss=0.013228  lr=3.200e-04\n",
      "Epoch 6 Step 20/32  loss=0.011922  lr=3.600e-04\n",
      "Epoch 6 completed in 34.3s  avg_loss=0.011693  lr=3.840e-04\n",
      "Epoch 7 Step 20/32  loss=0.010769  lr=4.240e-04\n",
      "Epoch 7 completed in 44.3s  avg_loss=0.010657  lr=4.480e-04\n",
      "Epoch 8 Step 20/32  loss=0.010092  lr=4.880e-04\n",
      "Epoch 8 completed in 43.6s  avg_loss=0.009975  lr=5.120e-04\n",
      "Epoch 9 Step 20/32  loss=0.009647  lr=5.520e-04\n",
      "Epoch 9 completed in 44.7s  avg_loss=0.009529  lr=5.760e-04\n",
      "Epoch 10 Step 20/32  loss=0.009179  lr=6.160e-04\n",
      "Epoch 10 completed in 43.1s  avg_loss=0.009128  lr=6.400e-04\n",
      "Epoch 11 Step 20/32  loss=0.008678  lr=6.800e-04\n",
      "Epoch 11 completed in 56.6s  avg_loss=0.008596  lr=7.040e-04\n",
      "Epoch 12 Step 20/32  loss=0.008381  lr=7.440e-04\n",
      "Epoch 12 completed in 56.7s  avg_loss=0.008307  lr=7.680e-04\n",
      "Epoch 13 Step 20/32  loss=0.008013  lr=8.080e-04\n",
      "Epoch 13 completed in 56.6s  avg_loss=0.007926  lr=8.320e-04\n",
      "Epoch 14 Step 20/32  loss=0.007502  lr=8.720e-04\n",
      "Epoch 14 completed in 56.7s  avg_loss=0.007418  lr=8.960e-04\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[134], line 17\u001b[0m\n\u001b[1;32m     10\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m SequentialLR(optimizer, schedulers\u001b[38;5;241m=\u001b[39m[warmup_scheduler, cosine_scheduler], milestones\u001b[38;5;241m=\u001b[39m[warmup_steps])\n\u001b[1;32m     12\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, optimizer, criterion, device,\n\u001b[1;32m     13\u001b[0m                   scheduler\u001b[38;5;241m=\u001b[39mscheduler, scheduler_step_per_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m                   clip_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[132], line 121\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, train_loader, epochs, val_loader, log_every, save_every)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# If scheduler provided but expects epoch stepping, user can pass scheduler_step_per_batch=False.\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 121\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_every\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;66;03m# If scheduler is provided and should be stepped per-epoch, step here\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[132], line 75\u001b[0m, in \u001b[0;36mTrainer._train_one_epoch\u001b[0;34m(self, train_loader, epoch, log_every)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 75\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m log_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     78\u001b[0m     avg \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "\n",
    "epochs = 20\n",
    "warmup_steps = 100\n",
    "total_steps = epochs * len(loader)\n",
    "min_lr = 0.0\n",
    "\n",
    "warmup_scheduler = LinearLR(optimizer, start_factor=1e-6, total_iters=warmup_steps)\n",
    "cosine_scheduler = CosineAnnealingLR(optimizer, T_max=max(1, total_steps - warmup_steps), eta_min=min_lr)\n",
    "scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[warmup_steps])\n",
    "\n",
    "trainer = Trainer(model, optimizer, criterion, device,\n",
    "                  scheduler=scheduler, scheduler_step_per_batch=True,\n",
    "                  clip_grad=1.0)\n",
    "\n",
    "\n",
    "history = trainer.fit(train_loader=loader, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f9ae7af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.006736\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAAAAAA/RjU9AAACDklEQVR4Ae3ZQW7DMAwEQLf//3OKHoM40rKhZaGYHmuK4g6DAIGPwx8BAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAYJnAY9lNH170VT7/FK1+vHzfhweqEz7F+7272uDDecvHa/O9xNs/YiXgabzdI37nO3+b73j/JO9+VWUecJRi9OyqycO+ccBxhvHTcJZLytKAswSz55cMnzRNAya9tqwJA84XNK+4J38Y8J7hOm7NAibrSWo6Ji72yAIWm+5UHgXMlpNVrQ4fBVw9VOd9AnZq3tHLBu9Q77wz2WD69ZjWdc4/7ZUETH8Up3XToToLkoCd9y3vJeBy8uYLbbAZdHm7aIPZ12NWtTphFHD1UJ33ZQGT5SQ1nZOHvbKAYbMdy8KA8/XMK+6JHwa8Z7iOW9OAswXNnnfM+qceacDJe8Bt8x1xwGHCffMVAg4SbpxvMPXZJ/70J+3O8eqv2F8i7h2vHvB4fp27e7yzj2H2v5dNZsdUESBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAv9V4AfK4xlGaTkl8QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=224x224>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCADgAOABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiivSPD3wO8a6/Zm6e1g0uP+AakzRO/JB+QKzLjH8QGcgjIrck/Zv8AFQSExarozOUzKGklUK248Kdh3DbtOTjkkY4yfP8AxX4D8R+DLgprOnSRwF9sd3H88EnLYw44BIUnacNjkgVzdFFFFFFFFFFFFFFFFFFFFFFFfT/wR+G9nomh2nim/h8zV76LzIN+CLaFvu7cE/M6kEt1AbbgfNu9goqOeCG6t5be4ijmglQpJHIoZXUjBBB4II4xXyR8X/h/D4E8SwnT/M/snUEaW2DsCYmU/PHnOSFypBPZgMkgk+d0UUUUUUUUUUUUUUUUUUUUVseE7G31PxlodheR+Za3WoW8MybiNyNIoYZHIyCelfc9FFFeb/HWxt7v4UalNPHvktJYJoDuI2OZFjJ46/K7Dn19cV8kUUUUUUUUUUUUUUUUUUUUUV0HgW//ALM8feH7w3f2SOPUIPNmMnlhYy4D7m7KVLA54wTnivt+iiivP/jXf/YfhRrG27+zzT+VBHiTY0m6RdyD1ygfI7ru7Zr5Aoooooooooooooooooooooor63+D3xAt/F/he3sLq63a7p8QS5SQnfKgOFlBJJfI27j/AHicgArn0iiivlj44/EC38V65b6TpF15+k6dktIhOyec8FhzhlUYCtju+CQQa8nooooooooooooooooooooooqxY395pl5HeWF3PaXUedk0Ehjdcgg4YcjIJH417B4e/aM13T7Mwa5pkGryD7k6SC2c8kncApU9QBgLjHOSc1tyftMwhITF4UkZymZQ1+FCtuPCnyzuG3acnHJIxxk+eeMvjD4p8X+bbfaf7N0x8r9jtGK71O4Ykf7z5VsEcKcA7RXn9FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFf/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAAAAAA/RjU9AAACC0lEQVR4Ae3Z204DMQwFwIX//2cQj73FJ5W3yYrhDcXr+IyritLj8EOAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECgTeCnrdMnGn3NXHITberJmVt6ayfGvIn3N8XEs71Dz3SLh3yId5GIYcCn8S4R8Tta98t8x+uTqPH5RVHAUYrR2fnj1zckAccZxqf1BCdXBAGrBNX5yQmK9kHAosPmx3XAekF1xUKEOuDC4TquLgMm60lqOoZ9p0cZ8J2mOz1TBcyWk1UtyV0FXDJU56UCdmqu6GWDK9Q77yw2mL49pnWdo2e9ioDh5+GN/3tRBMyUdq4ScOftJLPZYKK0c021wextNKta4lAFXDJU56VlwGQ5SU3n0DO9yoAzzXasrQPW66krFiavAy4cruPqIGC1oOq8Y8z3ewQBi7+k9853JAGHCTfPlwUcJNw932D0u5f904+028eb+p79IeIF4k0FPG6/zr1EvLuXYfbrwyazx1QRIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAwH8V+AXIgBlGaozMqQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=224x224>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCADgAOABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiitDRtD1TxDqKWGkWE97dNg7IUztBIG5j0VckZY4Azya7SP4HfEF0mZtFjjKJuVWvIcyHcBtXDkZwSecDCnnOAcPxD8OvF3hWzF5rGiTwWp6zIySonIHzMhIXJYAbsZ7Zrl6KKKKKKKKKKKKKKKKKKKKKKK7T4Z+AZvH/iU2bSyW+n2yCW8uEQkhc4CKcYDtzjPYMcHbg/XejaHpfh7TksNIsILK1XB2QpjcQANzHqzYAyxyTjk1oUV4H8Z/hJYwaXN4p8OWsdqbZAbyxt4sIyDA8xFUYUqOWHAIBbgg7vnyiiiiiiiiiiiiiiiiiiiiiivq/4CaNZ6f8M7bUYU/wBK1KWSW4kIGTsdo1UEDO0BcgHOCzeteoUUVHPBDdW8tvcRRzQSoUkjkUMrqRggg8EEcYr4Y8S6bDo3irV9Lt2kaCyvZreNpCCxVHKgnAAzgegrLooooooooooooooooooooor7D+DEzT/CTQXcRghJU+SNUGFmdRwoAzgcnqTknJJNd5RRRXxJ8QZmn+I3iR3EYI1O4T5I1QYWQqOFAGcDk9Sck5JJrm6KKKKKKKKKKKKKKKKKKKKK9s/Z88bQ6Vql14X1C4jit79xLZs5Cj7RwpTpyXULjJAygAGWr6Toori/if42h8E+Drq6S4jTVLhDFYRkjc0hwC4BBBCA7jkYOACRuFfGlFFFFFFFFFFFFFFFFFFFFFFFeweDfj9rui+VZ+IYv7XsVwvnZC3Ma/KM7ukmAGOGwzE8vXocf7RXg10mZrTWYyiblVrePMh3AbVxIRnBJ5wMKec4Bw/EP7SNmtmF8NaNO903WTUgFSPkfwoxL5G7+JccHnpXhmv+I9X8U6o2pa1fSXd2UCb2AUKo6BVUAKOpwAOST1JrLooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooor//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAAAAAA/RjU9AAACEklEQVR4Ae3Z0UoDQQwF0NX//2dFfCilnSRb46YDxydhspnck1Ioexz+CBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgUBb4Kle+TeFHcZK7aNWHir3/taw26128n3lqj/3r5MXmlUkf4u0UMQ/4NN4+ET+zTS/zHeuTrOeV51nAKEV0dmWG8K4kYJwhPg3vvewwDpglyM4vi7G+KA64fm6bkzBgvqC8YloiDDg9XMf9UcDKeio1HXO+3CMK+HLTd3owCFhbTq1qLnIQcG6ozpsF7NSc6GWDE+qdd643WP16rNZ1Tn2i1zpg/lP495pq3YmhOkvXATtvGewl4CB+y9U22MI42CTYYO3rsVY1lzAIODdU581RwMpyKjWd857uFQU83ewdHwgD5uvJK6ZDhwGnh+u4Pw6YLSg775jwjz3igMl7wA3yHUnAMOEO+dKAQcIt8gXz3z77T3/S7hGv+rL9IeIu8aoBj/vXufvEu30Ma/89bLL2mCoCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAg8F/gGxh0ZRkhjxAwAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=224x224>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCADgAOABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiitix8J+JNTs47yw8P6rd2smdk0FlJIjYJBwwGDggj8KsX/AIF8WaZ9pN54b1WOO13mab7I5jULnc28DaVGCdwOMc5xXP0UUUUUUUUUUUUUUUUUUUUUUUV9P/Df4I6XomnRX/imyg1DV5Pn+zy/vIbYEEbNv3ZG55JyAQNvTc3sFFeb/ED4PaF4vs7m6sLaDTtdbLpdRqVSVskkSqODuLHL43dDyBtPyhf2Nxpmo3NheR+XdWsrwzJuB2upIYZHBwQelV6KKKKKKKKKKKKKKKKKKKKK9I+B3h631/4lWz3Tfu9NiN+EwfndGUJyCMYZ1bvnbgjBr63ooor5o/aM8PW+n+KNO1yBsSapEyTpg8vEEAfJPdWUYAGNmeSTXi9FFFFFFFFFFFFFFFFFFFFFeyfs3yKPHWpxGGMu2mMwlJbcoEseVHOMHIJyCflGCOc/TdFFFeB/tMyKLfw1EYYy7PcsJSW3KAIsqOcYOQTkE/KMEc5+fKKKKKKKKKKKKKKKKKKKKKK6TwH4rm8GeMbDWUMhgR9l1Gmf3kLcOMZAJx8wBONyqT0r7XgnhureK4t5Y5oJUDxyRsGV1IyCCOCCOc1JRRXyB8YfGX/CX+Obj7NN5mmafm1tNrZR8H55Bhip3N0YYyqpnpXn9FFFFFFFFFFFFFFFFFFFFFFeifD/AOL+teBLf+zzBHqWk7y4tpXKtESDny352gsQSCCODjBJJ9zsfjr4Bu7OOebVJ7KRs5gntJC6YJHJRWXnrwT19eKsX/xr8A2P2lf7c+0TQbx5dvbyP5jLnhG27Dkjg7tp65xzXjHxA+OOqeK7O50jSbb+zNJmykjFszzpk8MRwqsNuVGehG4gkV5PRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRX//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAAAAAA/RjU9AAACFElEQVR4Ae3Z0WrDMAwF0Gz//88b20NpqGUpxIljOHsKtWzrHrFCybb5I0CAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECL4Gf19NaD195u7tohfr8xDsr0oZ38f46S3fc2X5+V9LuR7zlInYDNuMtFvG7M+Qw3xavdI6bs9QJ2EvRW5sTJLo1DtjP0F+NbpvweRgwS5CtT8jSvDIM2Kxe8MMoYD6gvOIRHFHARzQ3ookgYGU8lZoRLZ47Iwh47tAn7W4HrA2nVjU5bTvg5KZGXi/gSM0ZZ5ngDPWRdzYnWP16rNaNbPjoWc2A3V/BbzdU69623P7YDHh7FxdeKOCFuLccbYK3MF94SXuCta/HWtWFzVeObges7FykJghYGU6lZr5CEHB+Y6M6iALm48krRvV46pwo4KlDn7Q5DJgNKFt/SsgwYPIecJV8Wxywm3CZfL2AnYTr5OuE+P8nav6kXShe4Y37R8Sl4hUCbvvXuYvFO/Bd/jHJA3uVEiBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEDgs8AvDuhlGbR0hGgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=224x224>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=224x224>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "# pick sample index\n",
    "idx = 1\n",
    "inp, last, target = dataset[idx]        # inp: (C=num_input_frames,H,W), last/target: (1,H,W)\n",
    "\n",
    "# add batch dim and send to device\n",
    "inp = inp.unsqueeze(0).to(device)       # (1,C,H,W)\n",
    "last = last.unsqueeze(0).to(device)     # (1,1,H,W)\n",
    "target = target.unsqueeze(0).to(device) # (1,1,H,W)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    delta, mask = model(inp)             # predicted delta (1,1,H,W)\n",
    "    pred = last * (1.0 - mask) + (last + delta) * mask            # predicted next frame (1,1,H,W)\n",
    "\n",
    "\n",
    "\n",
    "# compute MSE\n",
    "loss = criterion(pred, target)\n",
    "print(f'MSE: {loss.item():.6f}')\n",
    "\n",
    "# display\n",
    "pred_img = pred.squeeze(0).squeeze(0).cpu()   # (H,W)\n",
    "target_img = target.squeeze(0).squeeze(0).cpu()\n",
    "for frame in inp.squeeze(0):\n",
    "    display(TF.to_pil_image(frame.cpu()))\n",
    "\n",
    "display(TF.to_pil_image(target_img))\n",
    "display(TF.to_pil_image(pred_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b53a90",
   "metadata": {},
   "source": [
    "### Predict next based on the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ddac5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
