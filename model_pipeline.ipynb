{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91e8ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import timm           # Huggingface Pretrained Models\n",
    "import imageio        # Saving GIFs\n",
    "import time \n",
    "import os\n",
    "import wandb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f09d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adcc669",
   "metadata": {},
   "source": [
    "## Encoder and Decoder Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31d9090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example UNet decoder\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, encoder_dim, out_channels=1):\n",
    "        super().__init__()\n",
    "\n",
    "        def up_block(in_channels, out_channels):\n",
    "            return nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True), # Out Dims: [B, out_channels, H*2, W*2]\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), # Out Dims: [B, out_channels, H*2, W*2]\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        \n",
    "        self.up1 = up_block(encoder_dim, 128)               # out dims: [B, 128, 32, 32]\n",
    "        self.up2 = up_block(128, 64)                        # out dims: [B, 64, 64, 64]\n",
    "        self.up3 = up_block(64, 32)                         # out dims: [B, 32, 128, 128]\n",
    "        self.up4 = up_block(32, 16)                         # out dims: [B, 16, 256, 256]\n",
    "\n",
    "        # final head: predict 2 channels -> (delta_raw, mask_logit)\n",
    "        self.final_conv1 = nn.Conv2d(16, 16, 3, padding=1) # out dims: [B, 16, 256, 256]\n",
    "        self.final_relu = nn.ReLU(inplace=True)\n",
    "        # output 2 channels (delta, mask)\n",
    "        self.final_conv2 = nn.Conv2d(16, 2, 1)               # out dims: [B, 2, 256, 256]\n",
    "\n",
    "        # zero-init final bias so initial delta/mask ~ 0\n",
    "        nn.init.zeros_(self.final_conv2.bias)\n",
    "\n",
    "        # learnable global scale for the delta (starts small)\n",
    "        self.delta_scale = nn.Parameter(torch.tensor(4.0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up1(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.up3(x)\n",
    "        x = self.up4(x)\n",
    "        x = self.final_conv1(x)\n",
    "        x = self.final_relu(x)\n",
    "        x = self.final_conv2(x)   # shape: [B, 2, H, W]\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViT_UNet_NextFrame(nn.Module):\n",
    "    def __init__(self, vit_model='vit_small_patch16_224', in_channels=3, out_channels=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = timm.create_model(vit_model, pretrained=True, features_only=True)\n",
    "        encoder_channels_dim = self.encoder.feature_info[-1]['num_chs'] # get last feature map channels\n",
    "        self.decoder = ConvDecoder(encoder_channels_dim, out_channels)\n",
    "        \n",
    "        \n",
    "        # Freeze whole encoder\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Find and unfreeze PatchEmbed robustly\n",
    "        for name, module in self.encoder.named_modules():\n",
    "            if module.__class__.__name__ == \"PatchEmbed\" or \"patch_embed\" in name:\n",
    "                for p in module.parameters():\n",
    "                    p.requires_grad = True\n",
    "                print(\"Unfroze patch embed:\", name)\n",
    "                break\n",
    "\n",
    "        # Unfreeze last block + norm\n",
    "        for name, p in self.encoder.named_parameters():\n",
    "            if name.startswith(\"model.blocks.11.\"):  # last block\n",
    "                p.requires_grad = True\n",
    "                print(\"Unfroze last block:\", name)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        feats = self.encoder(x)[-1]   # last feature map\n",
    "        out = self.decoder(feats)     # [B,2,H,W]\n",
    "        delta_raw = out[:, :1, ...]   # raw delta logits\n",
    "        mask_logit = out[:, 1:2, ...] # mask logits\n",
    "\n",
    "        # constrained delta + mask\n",
    "        delta = torch.tanh(delta_raw) * self.decoder.delta_scale\n",
    "        mask_prob = torch.sigmoid(mask_logit)   # in [0,1]\n",
    "\n",
    "        return delta, mask_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc40c4e3",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e03521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowDataset(Dataset):\n",
    "    def __init__(self, seq_dirs, input_len=3, pred_len=1, transform=None):\n",
    "        self.seq_dirs = seq_dirs\n",
    "        self.input_len = input_len\n",
    "        self.pred_len = pred_len\n",
    "        self.window_size = input_len + pred_len\n",
    "        self.transform = transform\n",
    "\n",
    "        # collect frame paths and index map\n",
    "        self.seq_frame_paths = []\n",
    "        for d in self.seq_dirs:\n",
    "            files = sorted([f for f in os.listdir(d) if f.endswith('.png')])\n",
    "            self.seq_frame_paths.append([os.path.join(d, f) for f in files])\n",
    "\n",
    "        self.index_map = []\n",
    "        for seq_index, paths in enumerate(self.seq_frame_paths):\n",
    "            L = len(paths)\n",
    "            max_start = L - self.window_size + 1\n",
    "            if max_start > 0:\n",
    "                for start in range(max_start):\n",
    "                    self.index_map.append((seq_index, start))\n",
    "\n",
    "    def __len__(self):\n",
    "        length = len(self.index_map)\n",
    "        return length\n",
    "\n",
    "    def _load_frame(self, path):\n",
    "        frame = read_image(path, mode=ImageReadMode.GRAY)\n",
    "        frame = frame.float() / 255.0  # Normalize to [0,1]\n",
    "        if self.transform is not None:\n",
    "            frame = self.transform(frame)\n",
    "        return frame\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq_idx, start_idx = self.index_map[idx]\n",
    "        paths = self.seq_frame_paths[seq_idx]\n",
    "\n",
    "        frames = [self._load_frame(paths[i]) for i in range(start_idx, start_idx + self.window_size)]\n",
    "        input_frame = torch.cat(frames[:self.input_len], dim=0)  # (C=input_len,H,W)\n",
    "        last_frame = frames[self.input_len - 1]                # (1,H,W)\n",
    "        target_frame = frames[self.input_len]                  # (1,H,W)\n",
    "\n",
    "        return input_frame, last_frame, target_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ecbb6f",
   "metadata": {},
   "source": [
    "## Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2741469",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, criterion, device,\n",
    "                 scheduler=None, warmup_steps=500, min_lr_fraction=0.0,\n",
    "                 total_steps=None, clip_grad=None, save_path=None,\n",
    "                 scheduler_step_per_batch=False,\n",
    "                 early_stopping_patience=None, early_stopping_min_delta=0.0,\n",
    "                 early_stopping_restore_best=True, wandb_run=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.min_lr_fraction = min_lr_fraction\n",
    "        self.total_steps = total_steps\n",
    "        self.clip_grad = clip_grad\n",
    "        self.save_path = save_path\n",
    "        self.scheduler_step_per_batch = scheduler_step_per_batch\n",
    "        self.wandb = wandb_run\n",
    "\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.early_stopping_min_delta = early_stopping_min_delta\n",
    "        self.early_stopping_restore_best = early_stopping_restore_best\n",
    "\n",
    "        self.global_step = 0\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "    def _save_checkpoint(self, tag=\"last\", is_best=False):\n",
    "        if not self.save_path:\n",
    "            return\n",
    "        state = {\n",
    "            \"model_state\": self.model.state_dict(),\n",
    "            \"optim_state\": self.optimizer.state_dict(),\n",
    "            \"scheduler_state\": self.scheduler.state_dict() if self.scheduler is not None else None,\n",
    "            \"global_step\": self.global_step,\n",
    "        }\n",
    "        torch.save(state, f\"{self.save_path}_{tag}.pt\")\n",
    "        if is_best:\n",
    "            torch.save(state, f\"{self.save_path}_best.pt\")\n",
    "\n",
    "        # after saving file to disk (e.g. in _save_checkpoint)\n",
    "        if self.wandb and os.path.exists(f\"{self.save_path}_{tag}.pt\"):\n",
    "            art = wandb.Artifact(f\"model-{run.name}\", type=\"model\")\n",
    "            art.add_file(f\"{self.save_path}_{tag}.pt\")\n",
    "            self.wandb.log_artifact(art)\n",
    "\n",
    "\n",
    "    def load(self, path, map_location=None):\n",
    "        ckpt = torch.load(path, map_location=map_location)\n",
    "        self.model.load_state_dict(ckpt[\"model_state\"])\n",
    "        self.optimizer.load_state_dict(ckpt[\"optim_state\"])\n",
    "        if self.scheduler is not None and ckpt.get(\"scheduler_state\") is not None:\n",
    "            try:\n",
    "                self.scheduler.load_state_dict(ckpt[\"scheduler_state\"])\n",
    "            except Exception:\n",
    "                pass\n",
    "        self.global_step = ckpt.get(\"global_step\", 0)\n",
    "\n",
    "    \n",
    "    def _current_lrs(self):\n",
    "        return [group['lr'] for group in self.optimizer.param_groups]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gradient_l1(pred, target):\n",
    "        dx_p = pred[:, :, :, 1:] - pred[:, :, :, :-1]\n",
    "        dy_p = pred[:, :, 1:, :] - pred[:, :, :-1, :]\n",
    "        dx_t = target[:, :, :, 1:] - target[:, :, :, :-1]\n",
    "        dy_t = target[:, :, 1:, :] - target[:, :, :-1, :]\n",
    "        return (dx_p - dx_t).abs().mean() + (dy_p - dy_t).abs().mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def _tv(x):\n",
    "        return (x[:, :, :, 1:] - x[:, :, :, :-1]).abs().mean() + (x[:, :, 1:, :] - x[:, :, :-1, :]).abs().mean()\n",
    "    \n",
    "    def _train_one_epoch(self, train_loader, epoch, log_every=50):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        start = time.time()\n",
    "\n",
    "        loader = tqdm(train_loader, desc=f\"Train E{epoch}\", leave=False)\n",
    "        \n",
    "        for i, (inputs, lasts, targets) in enumerate(loader):\n",
    "            inputs = inputs.to(self.device)\n",
    "            lasts = lasts.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            delta, mask = self.model(inputs)\n",
    "            \n",
    "            pred = lasts * (1.0 - mask) + (lasts + delta) * mask\n",
    "\n",
    "            target_delta = targets - lasts\n",
    "            pred_delta = pred - lasts\n",
    "            delta_loss = F.l1_loss(pred_delta, target_delta)\n",
    "\n",
    "            motion = target_delta.abs()\n",
    "            mask_target = (motion > 0.01).float()  # threshold tune\n",
    "            mask_bce = F.binary_cross_entropy(mask, mask_target)\n",
    "            \n",
    "            loss = 2 * delta_loss + 0.2 * mask_bce\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            total_norm = 0.0\n",
    "            for p in self.model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** 0.5\n",
    "\n",
    "            if self.clip_grad is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip_grad)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.scheduler is not None and self.scheduler_step_per_batch:\n",
    "                try:\n",
    "                    self.scheduler.step()\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            self.global_step += 1\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "            # Log metrics to wandb\n",
    "            if self.wandb:\n",
    "                self.wandb.log({\n",
    "                    \"train/loss\": loss.item(),\n",
    "                    \"train/mask_mean\": mask.mean().item(),\n",
    "                    \"train/delta_mean\": delta.abs().mean().item(),\n",
    "                    \"train/delta_std\": delta.std().item(),\n",
    "                    \"train/lr\": self._current_lrs()[0],\n",
    "                    \"train/grad_norm\": total_norm,\n",
    "                }, step=self.global_step)\n",
    "\n",
    "            # log example images occasionally (e.g. every 200 steps)\n",
    "            if self.wandb and (self.global_step % 200 == 0):\n",
    "                # select first example in batch\n",
    "                def to_img(t):\n",
    "                    return (t.squeeze(0).detach().cpu().numpy() * 255).astype(\"uint8\")\n",
    "                img_last = to_img(lasts[0])\n",
    "                img_pred = to_img(pred[0])\n",
    "                img_tgt  = to_img(targets[0])\n",
    "                self.wandb.log({\n",
    "                    \"train/visuals\": [\n",
    "                        wandb.Image(img_last, caption=\"last\"),\n",
    "                        wandb.Image(img_pred,  caption=\"pred\"),\n",
    "                        wandb.Image(img_tgt,   caption=\"target\"),\n",
    "                    ]\n",
    "                }, step=self.global_step)\n",
    "\n",
    "\n",
    "            if (i + 1) % log_every == 0:\n",
    "                avg = running_loss / float(i + 1)\n",
    "                lrs = self._current_lrs()\n",
    "                if len(lrs) == 1:\n",
    "                    lr_info = f\"lr={lrs[0]:.3e}\"\n",
    "                else:\n",
    "                    lr_info = \"lrs=\" + \",\".join(f\"{x:.3e}\" for x in lrs)\n",
    "                loader.set_postfix(loss=f\"{avg:.6f}\", lr=lr_info)\n",
    "                print(f\"Epoch {epoch} Step {i+1}/{len(train_loader)}  loss={avg:.6f}  {lr_info}\")\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        epoch_loss = running_loss / max(1, len(train_loader))\n",
    "        lrs = self._current_lrs()\n",
    "        if len(lrs) == 1:\n",
    "            lr_info = f\"lr={lrs[0]:.3e}\"\n",
    "        else:\n",
    "            lr_info = \"lrs=\" + \",\".join(f\"{x:.3e}\" for x in lrs)\n",
    "        print(f\"Epoch {epoch} completed in {elapsed:.1f}s  avg_loss={epoch_loss:.6f}  {lr_info}\")\n",
    "        return epoch_loss\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        total = 0.0\n",
    "        count = 0\n",
    "        for inputs, lasts, targets in tqdm(val_loader, desc=\"Val\", leave=False):\n",
    "            inputs = inputs.to(self.device)\n",
    "            lasts = lasts.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "            delta, mask = self.model(inputs)\n",
    "            pred = lasts * (1.0 - mask) + (lasts + delta) * mask\n",
    "            loss = self.criterion(pred, targets)\n",
    "            total += loss.item()\n",
    "            count += 1\n",
    "        return total / max(1, count)\n",
    "\n",
    "    def fit(self, train_loader, epochs=10, val_loader=None, log_every=50, save_every=1):\n",
    "        history = {\"train_loss\": [], \"val_loss\": []}\n",
    "        best_val = float('inf')\n",
    "        best_ckpt = None\n",
    "        no_improve = 0\n",
    "        best_epoch = None\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train_loss = self._train_one_epoch(train_loader, epoch, log_every=log_every)\n",
    "            history[\"train_loss\"].append(train_loss)\n",
    "\n",
    "            if self.scheduler is not None and not self.scheduler_step_per_batch:\n",
    "                try:\n",
    "                    self.scheduler.step()\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            if val_loader is not None:\n",
    "                val_loss = self.validate(val_loader)\n",
    "                history[\"val_loss\"].append(val_loss)\n",
    "                print(f\"Validation loss: {val_loss:.6f}\")\n",
    "\n",
    "\n",
    "                if self.wandb:\n",
    "                    self.wandb.log({\"val/loss\": val_loss}, step=self.global_step)\n",
    "\n",
    "                improved = (val_loss + self.early_stopping_min_delta) < best_val\n",
    "                if improved:\n",
    "                    best_val = val_loss\n",
    "                    no_improve = 0\n",
    "                    best_epoch = epoch\n",
    "                    # save best checkpoint\n",
    "                    if self.save_path:\n",
    "                        self._save_checkpoint(tag=f\"epoch{epoch}\", is_best=True)\n",
    "                    # keep best state in memory to optionally restore without file IO\n",
    "                    best_ckpt = {\n",
    "                        \"model_state\": self.model.state_dict(),\n",
    "                        \"optim_state\": self.optimizer.state_dict(),\n",
    "                        \"scheduler_state\": self.scheduler.state_dict() if self.scheduler is not None else None,\n",
    "                        \"global_step\": self.global_step,\n",
    "                    }\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "\n",
    "                if (self.early_stopping_patience is not None) and (no_improve >= self.early_stopping_patience):\n",
    "                    print(f\"Early stopping triggered (no improvement for {no_improve} epochs).\")\n",
    "                    break\n",
    "\n",
    "            if self.save_path and (epoch % save_every == 0 or epoch == epochs):\n",
    "                self._save_checkpoint(tag=f\"epoch{epoch}\")\n",
    "\n",
    "        if self.save_path:\n",
    "            self._save_checkpoint(tag=\"final\")\n",
    "\n",
    "        \n",
    "\n",
    "        if self.early_stopping_restore_best and best_ckpt is not None:\n",
    "            try:\n",
    "                self.model.load_state_dict(best_ckpt[\"model_state\"])\n",
    "                self.optimizer.load_state_dict(best_ckpt[\"optim_state\"])\n",
    "                if self.scheduler is not None and best_ckpt.get(\"scheduler_state\") is not None:\n",
    "                    try:\n",
    "                        self.scheduler.load_state_dict(best_ckpt[\"scheduler_state\"])\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                print(f\"Restored best model from epoch {best_epoch} with val_loss={best_val:.6f}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f5a3a8",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3360d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ec8958",
   "metadata": {},
   "source": [
    "### Initialize model, criterion and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf357f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfroze patch embed: model.patch_embed\n"
     ]
    }
   ],
   "source": [
    "# Model, loss, optimizer\n",
    "model = ViT_UNet_NextFrame(vit_model='timm/vit_small_patch16_dinov3.lvd1689m', in_channels=3, out_channels=1)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7c10a8",
   "metadata": {},
   "source": [
    "### Split Dataset, Initialize DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51373b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "\n",
    "seq_dirs = sorted(glob.glob(\"bouncing_balls_dataset/seq*\"))\n",
    "\n",
    "\n",
    "# deterministic shuffle + split (80/10/10)\n",
    "random.Random(42).shuffle(seq_dirs)\n",
    "n = len(seq_dirs)\n",
    "train_end = int(0.8 * n)\n",
    "val_end = train_end + int(0.1 * n)\n",
    "\n",
    "# Split directories\n",
    "train_dirs = seq_dirs[:train_end]\n",
    "val_dirs   = seq_dirs[train_end:val_end]\n",
    "test_dirs  = seq_dirs[val_end:]\n",
    "\n",
    "# Create datasets\n",
    "train_ds = SlidingWindowDataset(train_dirs, input_len=3, pred_len=1)\n",
    "val_ds   = SlidingWindowDataset(val_dirs, input_len=3, pred_len=1)\n",
    "test_ds  = SlidingWindowDataset(test_dirs, input_len=3, pred_len=1)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed577675",
   "metadata": {},
   "source": [
    "## Wandb Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5947fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_config = dict(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=LR,\n",
    "    epochs=EPOCHS,\n",
    "    input_len=3,\n",
    ")\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"video-generation-project\",\n",
    "    job_type=\"train\",\n",
    "    config=wandb_config,\n",
    "    save_code=True\n",
    ")\n",
    "\n",
    "wandb.watch(model, log=\"all\", log_freq=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce031582",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb317039",
   "metadata": {},
   "outputs": [],
   "source": [
    "WARMUP_STEPS = len(train_loader) * 1  # 1 epoch warmup\n",
    "TOTAL_STEPS = EPOCHS * len(train_loader)\n",
    "MIN_LR = 0.0\n",
    "\n",
    "warmup_scheduler = LinearLR(optimizer, start_factor=1e-6, total_iters=WARMUP_STEPS)\n",
    "cosine_scheduler = CosineAnnealingLR(optimizer, T_max=max(1, TOTAL_STEPS - WARMUP_STEPS), eta_min=MIN_LR)\n",
    "scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[WARMUP_STEPS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea75754",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, optimizer, criterion, device,\n",
    "                  scheduler=scheduler, scheduler_step_per_batch=True,\n",
    "                  clip_grad=1.0, wandb_run=run)\n",
    "\n",
    "history = trainer.fit(train_loader=train_loader, val_loader=val_loader,\n",
    "                      epochs=EPOCHS, log_every=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23ca642",
   "metadata": {},
   "source": [
    "## Testing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f9535ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "def generate_recursive(model, init_inputs, steps, device, cv_enhance=False):\n",
    "    \"\"\"\n",
    "    model: your ViT_UNet_NextFrame (expects input shape (B,C,H,W))\n",
    "    init_inputs: tensor (B,C,H,W) with C = input_len (use unsqueeze(0) for single example)\n",
    "    steps: number of future frames to generate\n",
    "    returns: list of predicted frames (as tensors, each (H,W) in [0,1])\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    frames = []\n",
    "    cur = init_inputs.to(device).clone()\n",
    "\n",
    "    if cv_enhance:\n",
    "        kernel = np.ones((2, 2), np.uint8)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(steps):\n",
    "            delta, mask = model(cur)\n",
    "            last = cur[:, -1:, :, :]\n",
    "            pred = last * (1.0 - mask) + (last + delta) * mask\n",
    "\n",
    "            if cv_enhance:\n",
    "                # morph close on binarized prediction\n",
    "                pred_np = pred[0,0].detach().cpu().numpy()\n",
    "                pred_np = cv2.medianBlur(pred_np.astype(np.float32), 3)\n",
    "                pred_bin = (pred_np > 0.2).astype(np.uint8)\n",
    "                \n",
    "                # after pred_bin computed\n",
    "                num, labels, stats, _ = cv2.connectedComponentsWithStats(pred_bin, connectivity=4)\n",
    "                # stats: [label, x, y, w, h, area]\n",
    "                min_area = 200  # tune\n",
    "                mask = np.zeros_like(pred_bin)\n",
    "                for i in range(1, num):  # skip background\n",
    "                    if stats[i, cv2.CC_STAT_AREA] >= min_area:\n",
    "                        mask[labels == i] = 1\n",
    "                pred_clean = mask.astype(np.uint8)\n",
    "    \n",
    "                pred = torch.from_numpy(pred_clean).float().to(device).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            frames.append(pred[0].squeeze(0).cpu())\n",
    "            cur = torch.cat([cur[:, 1:, :, :], pred], dim=1)\n",
    "    return frames\n",
    "\n",
    "\n",
    "def save_frames_as_gif(frames, path, fps=10, loop=0):\n",
    "    \"\"\"\n",
    "    frames: list of (H,W) tensors in [0,1]\n",
    "    path: output filepath (e.g. 'pred.gif')\n",
    "    fps: frames per second\n",
    "    loop: number of loops (0 = infinite)\n",
    "    \"\"\"\n",
    "    \n",
    "    out = []\n",
    "    for frame in frames:\n",
    "        # Convert tensor to numpy array and scale to [0,255]\n",
    "        arr = (frame.squeeze().cpu().numpy() * 255).astype('uint8')\n",
    "        out.append(arr)\n",
    "    imageio.mimsave(path, out, format='GIF', fps=fps, loop=loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97127d0",
   "metadata": {},
   "source": [
    "### Generate video in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af054015",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"model.pt\", map_location=device)\n",
    "model = ViT_UNet_NextFrame(vit_model='timm/vit_small_patch16_dinov3.lvd1689m', in_channels=3, out_channels=1).to(device)\n",
    "model.load_state_dict(ckpt)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "13736b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "steps = 20\n",
    "idx = 5705\n",
    "init_input, _, _ = test_ds[idx]\n",
    "\n",
    "frames = generate_recursive(model, init_input.unsqueeze(0), steps, device, cv_enhance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "201424dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAFsFJREFUeJzt3X9sVfX9x/HXufT2tthiLRQoAraz/Ci2YFFAp9bU4gaI00VFGQyXLsSxLDPZ3ByZWaJism5xZHMLc4MMDepwnUNkG4quCp1kIgKxxaENtcMig/6gQH/ce9t7vn8wP/syIVyEez/n3Pt8JE1sqee8U+15cj7n3HMd13VdAQAgKWB7AACAdxAFAIBBFAAABlEAABhEAQBgEAUAgEEUAAAGUQAAGBnxfqPjOImcAwCQYPG8VpkzBQCAQRQAAAZRAAAYRAEAYBAFAIBBFAAABlEAABhEAQBgEAUAgEEUAAAGUQAAGEQBAGAQBQCAQRQAAAZRAAAYRAEAYBAFAIBBFAAABlEAABhEAQBgEAUAgEEUAAAGUQAAGEQBAGAQBQCAQRQAAAZRAAAYRAEAYBAFAIBBFAAABlEAABhEAQBgEAUAgEEUAAAGUQAAGEQBAGAQBQCAQRQAAAZRAAAYRAEAYBAFAIBBFAAABlEAABhEAQBgEAUAgEEUAAAGUQAAGEQBAGAQBQCAQRQAAAZRAAAYRAEAYBAFAIBBFAAABlEAABhEAQBgEAUAgEEUAAAGUQAAGEQBAGAQBQCAQRQAAEaG7QEAeFdVVZUmTJhwzv9eZ2en6urqEjAREs1xXdeN6xsdJ9GzALAgOzv7jL/fq1ev1sKFC895m42Njbrmmmt0psNLb2/vOW8T5y+ewz1RANKY4zjavn27SkpKTvvnOTk5CoVC57zdgYEBdXd3n/bP+vr6dOWVV6qjo+Oct4vzE8/hnuUjIA0tXbpURUVFchxHJSUlGj58+AXdfkZGxhm3GY1G9dBDD6m3t1cnTpzQT3/6Uw0MDFzQ/eOz40wBSBN5eXkaOnSoJOmPf/yjrrnmGssTSUeOHNGsWbMUDocVjUZ15MgR2yOlNJaPABhPPfWU7r77bklSMBhUIGD/5kPXdRWJRCRJTU1NuuqqqyxPlNqIApDmFixYoFtvvVWSdN1116m4uNjyRGd29OhRbdq0SdLJC9W1tbWWJ0o9cR3u3ThJ4oMPPnzyEQgE3CuuuMJds2ZNvL/invLmm2+6U6dOdUOhkPWfZSp9xIMzBSAFXXTRRWptbVV+fr4vf3dd15XruiovL9fevXttj5My4jnc219UBHBB3XbbbfrrX/+qYcOG+TII0sm/hDqOo7Vr16q+vl4vv/yy8vLybI+VFrglFUgBBQUF5iLtzTffrBtuuMHyROfPcRzNmDFDkhSJRDRnzhy9/fbbam5utjxZamP5CPA5x3F0yy236KWXXrI9SsI9/PDDevjhh+O7YIpPiefnRhQAn1u3bp1uvPFGjR071vYoCdfe3q59+/apurpa4XDY9ji+wzUFIIUNHz5cS5cu1cyZM9MiCJI0YsQIlZeX6+tf/7omTpxoe5yUxJkC4EOZmZm6+uqr1dDQkLa/m8uWLdPatWvV399vexTfYPkISFFPPvmk7rrrLuXl5aXt7+bx48e1Z88eVVZWco0hTiwfASlq2LBhuuSSS9I2CJKUm5urSZMm6ZFHHtHll19ue5yUQRQAHwkEAho9erSys7Ntj+IJBQUFeuihhzRjxgzl5+fbHiclsHwE+MjYsWP13nvvKTs7W0OGDLE9jmdEIhG99tprmjdvnu1RPC2ewz0vXgN84u6779Zdd92loUOHeuIJp16SmZmpYDBoe4yUwP9ZgE9cddVVuuOOOwjCGeTk5Ki8vPwzvVMc/ov/uwCkhFmzZmn37t2aMGGC7VF8jSgAHpedna0NGzZowYIFtkfxtE8eord69Wp9//vftz2Ob3FNAfCwcePGafr06brpppuUm5trexzPcxxHs2bN0nvvvWd7FN/iTAHwsC996UvasGEDQThHn5w14NwRBQAp57bbbtOuXbs0YsQI26P4DstHAFJOXl6esrKylJHBIe5ccaYAeFQoFOLeeyQdGQU8KBAIaOvWrSotLbU9CtIMZwqAR+Xn53OBGUlHFAAABlEAABhEAfCYK6+8UmvWrNHIkSNtj+JrwWBQK1eu1Je//GXbo/gKUQA8Zvz48fra176mYcOG2R7F14YMGaJ77rlHFRUVtkfxFaIAADCIAoCUtnjxYtXV1SkzM9P2KL7A6xQApLTi4mLl5OTwTnVx4kwBAGAQBQCAQRQAAAZRAAAYRAEAYBAFAIBBFAAABlEAABhEAUDKy83N1apVq3TDDTfYHsXziAKAlJeVlaV7771XEydOtD2K5xEFAIBBFACP2b59u+bOnauDBw/aHiVldHd369Zbb9XmzZttj+J5juu6blzf6DiJngXAfwQCAe3bt08lJSW2R0kJR44c0WWXXaa+vj7bo1gVz+GeMwXAg/hLGGwhCoDHVFVVaffu3Ro3bpztUZCGiALgMbm5uSorK1MoFLI9SsrIysrSokWLNGHCBNujeB5RAJDycnNz9dvf/laVlZW2R/E8ogAAMIgC4DHvv/++VqxYoc7OTtujpIyenh79+Mc/1u7du22P4nnckgp4ELekXjg9PT06cOCAKioq1N/fb3scq7glFUDae+KJJwjCOSAKAFJaNBolCOeAKAAADKIAADCIAuBBsVhMNTU1+tWvfmV7FN+KRCJauHChnn76aduj+EqG7QEAnN62bds0fPhwlZeX69prr1UwGLQ9kq/EYjG9/vrrOnTokO1RfIVbUgGPy8nJUWtrq/Lz822P4huu6yocDqu4uJgo/D/ckgogLb300kuaOXOm2tvbbY/iO0QB8LhoNKrnn39eTU1Ntkfxjc7OTr377rsaGBiwPYrvEAXA48LhsJYtW6a//OUvtkfxPNd1FYlEiMF54EIzgJTyxS9+Ubt27bI9hm9xpgAgpbS3t6u7u9v2GL5FFACf6OnpUWdnZ1x3kKSjSCSi9vZ2lo7OE7ekAj4RDAZVXFysxsZGXrNwGvX19Zo/f776+voI5xlwSyqQQqLRqNra2vTNb35TO3bssD2Op/z85z/X448/rt7eXoJwnrjQDPhIT0+PVq9erSlTpmjkyJEaP358Wp/Fh8Nhffjhh/rDH/6gv//977bHSQksHwE+5DiOpk+frh07dqT17+bevXs1depUDQ4O2h7FF+I53HOmAPiQ67ppv0yycuVKvfDCCwThAuOaAuBTx48f1xtvvKGjR4/aHiWpIpGItm7dqi1btqihocH2OCmH5SPA57Zu3arrr78+LX5HXdfV4cOHVVRUxLupfQbcfQSkgZqaGt1///22x0iKJ598UtXV1QqHw7ZHSVlcUwB8rrm5Wdu2bdMzzzwjSZo4caJmzJhheaoLa3BwUBs2bNCWLVt4MGCCsXwEpJj77rtPv/jFLxQMBlPi93ZwcFA9PT0qKSnRkSNHbI/jaywfAWlo3bp1KisrS5kL0HV1dSotLVVHR4ftUdICy0dAiunp6VFra6tWrlypuXPn6tprr7U90jlraWnR+vXrJUk7d+7UwYMHLU+UPogCkIIikYgeffRRSSevMUgn39YzFArZHCsux44d0zvvvKPly5fbHiUtcU0BSGHBYNA8PO93v/udFixYYHmis5s/f75effVV7jBKgHgO90QBSBPV1dWaMGGCJOk73/mO+WcvePrpp7V9+3ZJJ99fua2tzfJEqYkoADitdevWmdtWi4qKlJmZmbR9x2IxtbS0nPJ4iuXLl+uFF15I2gzpiigAOC3HceQ4jgKBgBobGzVp0qSk7bu3t1ef+9znTrm9NBaLJW3/6YwH4gE4rU8eqBeLxfSNb3xDF110kfmzJUuWXNBrD++8845+9KMfmc9jsZi6uroIgUcRBSDNvf7666d8PmbMGBUWFl6w7b/11lv685//fMG2h8Ri+QgA0gSvaAYAnBOiAAAwiAIAwCAKAACDKAAADKIAADCIAgDAIAoAAIMoAAAMogAAMIgCAMAgCgAAgygAAAyiAAAwiAIAwCAKAACDKAAADKIAADCIAgDAIAoAAIMoAAAMogAAMIgCAMAgCgAAgygAAAyiAAAwiAIAwCAKAACDKAAADKIAADCIAgDAIAoAAIMoAAAMogAAMIgCAMAgCgAAgygAAAyiAAAwiAIAwCAKAACDKAAADKIAADCIAgDAIAoAAIMoAAAMogAAMIgCAMAgCgAAgygAAAyiAAAwiAIAwCAKAACDKAAADKIAADCIAgDAIAoAAIMoAAAMogAAMIgCAMAgCgAAgygAAAyiAAAwiAIAwCAKAACDKAAADKIAADCIAgDAIAoAAIMoAAAMogAAMIgCAMAgCgAAgygAAAyiAAAwiAIAwCAKAACDKAAADKIAADCIAgDAyLA9AAAkQiAQ0E033aRQKJSwfbS1tWn37t0J274Njuu6blzf6DiJngUAzlsgcHIBJBQKaf/+/Ro9enTC9vXss8/qq1/96ilfi8ViCdvf+YrncE8UAKSUTZs2aeLEiXIcR0VFRcrISNyCyPHjx3Xo0CHz+dGjR3XjjTeqr68vYfs8H/Ec7lk+AuBbl1xyie655x7zueM4qqio0JgxY5Ky/9zcXOXm5prPT5w4ofvuu0/hcFjd3d169tlnkzLHhcSZAgBfCoVCKisr01tvvWWWjLykublZ06dPVywW0+DgoPr7+22PFNeZgvd+kgAQh5/85Cd69dVXPfsX1uLiYn344YdqbW1VXV2d7XHixpkCAN+orq7W7NmzJUlz587VtGnTLE8UnwMHDuiZZ57RE088oYMHD1qbgwvNAFLG6NGj9cADD+i73/2u7VE+E9d1NWfOHDU2NkqSDh06lPQ7lYgCgJQQCoXU3NyswsJCDRkyxPY4n1k0GpXruopEIpo8ebLa2tqSun/uPgLga7W1tRo3bpwCgYBGjBjh6yBIUjAYlCRlZGTol7/8pZ5//nk999xzlqc6FVEA4Dm5ubkqKSnRHXfcocsvv9z2OBdcIBDQ7bffrqNHj6qxsVFNTU2eedEby0cAPGf27Nl65ZVXJKX2scd1XXV1demyyy7TiRMnkrK/s+GWVACe8rOf/UyPP/64HMdJ6SBIJ4OXm5urzZs36/bbb7c9jiSWjwB4RE5OjiorK1VVVaWpU6faHidpgsGgrrvuOu3Zs0ddXV164403rM7D8hEA6xzHUVlZmfbs2ZPWx5rm5maVlpZqYGAgIdtn+QiALzz22GPauHGj7TGsGz9+vBobG1VVVWVtBpaPAFg3evRoFRUV2R7DuszMTE2aNEk5OTnWZuBMAYBVQ4cOTejjrf0oFAopKyvLyr65pgDAmoKCAu3atUvDhw+3dhD0omPHjqmhoUG33HLLBd0u1xQAeFZ1dbV++MMfatSoUQThfwwbNkzl5eV67LHHdOmllyZ130QBgBWVlZW6//77WTo6g3Hjxmn58uUJfTvR0yEKAACDKACAhz3yyCP61re+lbT9EQUASeU4jqZMmaJRo0bZHsXzHMfRvHnzdP311ydtnyzmAUiqrKws1dfXq6CgwPYoOA3OFAAkzbx587R582bl5eVxm/s5qKqq0pYtW5ISUs4UACRNYWGhKisrbY/hOyNHjlRlZaVCoVDC98WZAoCkSIdHYacCzhQAJMWmTZtUUVFhewycBWcKABLOcRwVFxersLDQ9ii+FQgE9JWvfEUzZ85M7H4SunUA+I/+/n5FIhHbY/hWRkaGamtrdeeddyZ0P0QBQMK5rqvq6mo9+OCDtkfBWRAFAEmxePFiVVdX2x4DZ8GFZgAJ5ziOli1bptLSUtuj4Cw4UwAAGEQBQEKVlZVp7dq1GjNmjO1REAeiACChxowZoyVLlujiiy+2PUpKKCgo0JQpUxQIJObwTRQAwEfuvfdebdu2TUOHDk3I9okCAPhIoh8XQhQAAAZRAJBQrusqFovJdV3boyAORAFAQr355psqKyvT/v37bY+COBAFAAnV09Ojf/7znzz3yCd4RTOAhAoEAsrKyuK9FHyCMwUACVVZWamWlhaVlJTYHgVx4EwBQEJlZmZq5MiRtsdAnDhTAAAf6enp0eHDhxN2NxdRAAAf+c1vfqNp06app6cnIdsnCgDgIwMDAwqHwwnbPlEAABhEAQBgEAUACfX222/r5ptv1r/+9S/boyAORAFAQnV2duq1115L2IVRXFi8TgEAfOCTW1AT/WBBogAAPjAwMKC5c+eqsbExofth+QgAfMB1Xe3bt0///ve/E7ofogAgKcLhsKLRqO0xcBZEAUDCua6r2bNn68EHH7Q9Cs6CawoAkqKjo0PHjh2zPYYvNTY2av369Un5+XGmACBp+vr6dPjwYcViMduj+EpTU5NWrFhBFACklvXr12vy5Mnq6OiwPQrOgOUjAEkzODio/v7+hN9rnypc19Wjjz6q+vr6pO2TKABIqlgspvfff1+O46igoMD2OJ63ceNG7dy5M2n7Y/kIQFKFw2FVVlZq1apVtkfBaRAFAEnH8tHZNTU1ac6cOfrggw+Sul+iAMCK/fv3q6GhQYODg7ZH8aSuri698sorSb+NlygAsOKpp57SnXfeqUgkwpnD/7D58yAKAKzp6OjQ1VdfrRdffNH2KJ7y7W9/W0uWLLGyb+4+AmDNwMCA9u7dq66uLtujeMKxY8f0pz/9SQ0NDWppabEyA1EAYF00GlU4HFZmZqYcx7E9jhXRaFQHDhxQTU2N1Vd8O26ci1fp+h8KQOJdfPHFqqio0N/+9re0Pdb84Ac/0Jo1a9Te3p6wfcRzuOeaAgDruru7tXfvXq1YscLasoktfX19qq2tVX19fUKDEC/OFAB4Sl1dnaqrq5WXl2d7lITr7e1VW1ubKioqkvIe1pwpAPCdRYsWafHixbbHSIpf//rXmjZtWlKCEC+iAMBTwuGwdu3apaVLl6qtrc32OAkxMDCg733ve/r973+vvr4+2+OcguUjAJ61ceNGlZSUyHEclZSUKCPD/zdMdnd3q7W1VV/4whcS/n7L/yuewz1RAOBZnxx3srKy1NLSolGjRlme6Pw999xzWrRokZVXLcezT/9nF0DK+uQgFg6HtXDhQtXU1PjyekNjY6MeeOABSdLHH3/s6cd6EAUAnheLxVRfX6/x48fr0ksvlSRNnjxZhYWFlic7s97eXv3jH/+QJL377rt6+eWXLU8UH5aPAPjS6tWrVVNTYz730jHKdV198MEHKi0t9dT7UXNNAUDKGjt2rPLz8yVJq1at0uc//3nLE/3XihUrtG7dOu3bt8/2KKfgmgKAlPXRRx/po48+kiS9+OKL2r9/vyRp/vz5SX3h2+DgoDZs2HDKraX19fWeC0K8OFMAkFJ27typK6644lNfvxAP24tGo59aDopEIpo8ebIOHjx4XttOBpaPAKSdESNGKBgMnvK1/Px87dixQ9nZ2ee17UWLFqm+vv6Ur7muq8OHD3vq2sGZsHwEIO2c7qFy3d3dqq2t/VQsztWOHTv08ccfn9c2vI4zBQBIEzwQDwBwTogCAMAgCgAAgygAAAyiAAAwiAIAwCAKAACDKAAADKIAADCIAgDAIAoAAIMoAAAMogAAMIgCAMAgCgAAgygAAAyiAAAwiAIAwCAKAACDKAAADKIAADCIAgDAIAoAAIMoAAAMogAAMIgCAMAgCgAAgygAAAyiAAAwiAIAwCAKAACDKAAADKIAADCIAgDAIAoAAIMoAAAMogAAMIgCAMAgCgAAgygAAAyiAAAwiAIAwCAKAACDKAAADKIAADCIAgDAIAoAAIMoAAAMogAAMIgCAMAgCgAAgygAAIyMeL/Rdd1EzgEA8ADOFAAABlEAABhEAQBgEAUAgEEUAAAGUQAAGEQBAGAQBQCAQRQAAMb/AeKQsa9KFjyIAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for frame in frames[:100]:\n",
    "    clear_output(wait=True)\n",
    "    plt.imshow(frame.squeeze(0), cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis('off')\n",
    "    display(plt.gcf())\n",
    "    plt.pause(0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e1d497",
   "metadata": {},
   "source": [
    "### Save to gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6c55dd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_frames_as_gif(frames, \"example3.gif\", fps=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
